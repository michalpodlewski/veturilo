{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Goal, assumptions and description of the approach\n",
    "\n",
    "Goal of the tool is to suggests number of bikes to be supplied to a given station in the morning to minimise the imbalance between supply and demand of bikes\n",
    "\n",
    "The challenging aspect of the task is that there is no way of knowing the \"true\" demand for bikes - we only know how many bikes were used in given time but this is limited by the actual supply of bikes.\n",
    "While it is not possible to know the true demand my proposition is to estimate the imbalance using the difference of prediction created by two models:\n",
    "one using \"global\", seasonal features and other using \"local\" features (numbers of rentals in a few rolling windows within last days).\n",
    "\n",
    "If there is a huge disproportion between prediction based on local parameters and more global ones it might mean, that some demand for bikes have not been served. \n",
    "\n",
    "Of course to prove or disprove this statement one would need to conduct an experiment on the ground.\n",
    "\n",
    "Different approach would be to use the number of available bikes as a measure of supply but is very hard to accurately predict on hourly level (and on the other hand: there is no obvious way to aggregate number of available bikes to a daily dataset). This approach could be of course examined if any serious drawbacks of current approach were found.\n",
    "\n",
    "Also worth noting is that current approach uses daily predictions which would be easier to use by the teams supplying the bikes to the stations.\n",
    "\n",
    "In the current implementation the \"**global**\" model is a set of decision trees trained individually for each station. The choice od a decision tree is dictated by non-linear character of relation between rent count and calendar features.\n",
    "\n",
    "As this is a PoC a few ways of further development can be thought of:\n",
    "\n",
    "- Clustering stations according to their usage patterns and train more sophisticated models (i.e. XGB regressor or an LSTM) on per-cluster basis. Currently using such models per station or using station number as a one-hot encoded feature would demand either way more time or computing power\n",
    "\n",
    "- Using additional features, one obvious example that comes to mind is a weather forecast\n",
    "\n",
    "\"**Local**\" model in current implementation is a Linear Regression computed per station using number of rentals from last day as a whole, same hour 24 hours ago and same day of week a week ago. Choice of linear regression is dictated by the belief that relation between neighbouring rent counts should be linear.\n",
    "\n",
    "Final recomendation is based on the difference between prediction of local and global model. \n",
    "\n",
    "Note: functions used in the process are documented separately (in docstrings) and are not described thoroughly in this document\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error\n",
    "import veturilo_helper as vh\n",
    "import veturilo_timeseries_function as vtf\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "hourly_rentals = vh.get_hourly_rentals_df()\n",
    "hourly_rentals = hourly_rentals[hourly_rentals[\"uid\"] != -1]\n",
    "hourly_rentals[\"D\"] = hourly_rentals[\"dt\"].dt.to_period(\"D\")\n",
    "\n",
    "hourly_rentals = vtf.extract_features(hourly_rentals, rolsum_column=\"rent_count\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/podles/projects/veturilo/veturilo_timeseries_function.py:82: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  df.loc[:, \"weeknum\"] = df.loc[:, \"dt\"].dt.week\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spliting dataset into train and test\n",
    "\n",
    "Models are trained on the period 2018-03-01 - 2019-11-30 and tested on the period 2020-06-01 - 2020-11-31\n",
    "\n",
    "It's worth noting that year 2020 had it's particularities (Veturilo system was shut down for whole month of April and the COVID-19 pandemic might have not yet known implications to users' behavior\n",
    "\n",
    "Due to limited timeframe experiments with more sophisticated crossvalidation techniques were not conducted"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train = hourly_rentals[hourly_rentals[\"dt\"] < \"2020-01-01\"]\n",
    "test = hourly_rentals[\n",
    "    (hourly_rentals[\"dt\"] > \"2020-06-15\") & (hourly_rentals[\"dt\"] < \"2021-01-01\")\n",
    "]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# solving the cold start problem is left outside the scope of this PoC\n",
    "# but scale of the problem is not that big (it affects only week of the history of each station)\n",
    "# for now the data containing NaNs in predictions are removed\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the models\n",
    "\n",
    "Features used in global model, trained on seasonalities are:\n",
    "\n",
    "- ISO number of week within a year\n",
    "- Day of the week\n",
    "- Hour of the day\n",
    "\n",
    "Features used in local models are:\n",
    "\n",
    "- Number of rentals within 24 hours but with 24 hours offset (so the *youngest* datapoint used is 24hours old\n",
    "- Number of rentals within 24 hour a week before\n",
    "- Number of rentals within 1 hour a day before\n",
    "\n",
    "The features are selected in a way that enables action with reasonable lead time to supply bikes in the morning\n",
    "\n",
    "(Almost) no hyperparameter tuning have been performed, as this is only a PoC. Only quick-win used is changing criterion for DT to absolute error, as it fit's exactly our purpose (and indeed yields better result than MSE)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "features_list_DT = [\"weeknum\", \"dayofweek\", \"hour\"]\n",
    "features_list_lin = [\"rent_count_48_24\", \"rent_count_25_24\", \"rent_count_168_144\"]\n",
    "\n",
    "\n",
    "DT_model_directory = {\n",
    "    u: vtf.create_model(\n",
    "        train[train[\"uid\"] == u],\n",
    "        tgt=\"rent_count\",\n",
    "        features=features_list_DT,\n",
    "        mdl_fun=DecisionTreeRegressor,\n",
    "        max_depth=4,\n",
    "        criterion=\"mae\" \n",
    "    )\n",
    "    for u in train[\"uid\"].unique()\n",
    "}\n",
    "\n",
    "lin_model_directory = {\n",
    "    u: vtf.create_model(\n",
    "        train[train[\"uid\"] == u],\n",
    "        tgt=\"rent_count\",\n",
    "        features=features_list_lin,\n",
    "        mdl_fun=LinearRegression,\n",
    "    )\n",
    "    for u in train[\"uid\"].unique()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"global_prediction\": {\n",
    "        \"model_directory\": DT_model_directory,\n",
    "        \"features_list\": features_list_DT,\n",
    "    },\n",
    "    \"local_prediction\": {\n",
    "        \"model_directory\": lin_model_directory,\n",
    "        \"features_list\": features_list_lin,\n",
    "    },\n",
    "}\n",
    "train = vtf.add_predictions(train, params)\n",
    "test = vtf.add_predictions(test, params)\n",
    "\n",
    "daily_train = vtf.aggregate_daily_predictions(train)\n",
    "daily_test = vtf.aggregate_daily_predictions(test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validating quality of prediction\n",
    "\n",
    "Quality of hourly prediction of both models is very weak but as our goal is to act using the aggregated, daily imbalances th import fact is that aggregated daily predictions are decent enough for the PoC\n",
    "\n",
    "Aggregated predictions don't suffer much from overfitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def prediction_quality_params(df,fun=explained_variance_score):\n",
    "    return {\n",
    "        \"Local prediction\" : fun(df[\"rent_count\"], df[\"local_prediction\"]),\n",
    "        \"Global prediction\" : fun(df[\"rent_count\"], df[\"global_prediction\"])\n",
    "        }\n",
    "\n",
    "def show_prediction_quality(df_test,df_train,fun=explained_variance_score):\n",
    "    train_prediction_quality = prediction_quality_params(df_train,fun)\n",
    "    test_prediction_quality = prediction_quality_params(df_test,fun)\n",
    "\n",
    "    print(\"explained variance for test set: \")\n",
    "    print(f\"Local prediction {test_prediction_quality['Local prediction']:.2f}\")\n",
    "    print(f\"Global prediction {test_prediction_quality['Global prediction']:.2f}\")\n",
    "\n",
    "    print(\"explained variance for train set: \")\n",
    "    print(f\"Local prediction {train_prediction_quality['Local prediction']:.2f}\")\n",
    "    print(f\"Global prediction {train_prediction_quality['Global prediction']:.2f}\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction quality for daily aggregates:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "show_prediction_quality(daily_test,daily_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "explained variance for test set: \n",
      "Local prediction 0.72\n",
      "Global prediction 0.57\n",
      "explained variance for train set: \n",
      "Local prediction 0.74\n",
      "Global prediction 0.59\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction quality for hourly datasets:\n",
    "\n",
    "As it was mentioned before - quality of hourly prediction is poor and should not be used \"as is\" (only after aggregation)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "show_prediction_quality(test,train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "explained variance for test set: \n",
      "Local prediction 0.21\n",
      "Global prediction 0.23\n",
      "explained variance for train set: \n",
      "Local prediction 0.28\n",
      "Global prediction 0.31\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final bike supply recommender\n",
    "\n",
    "According to our assumption final recommendation of number of bikes supplied to given station on a given day should be a function of difference between predictions of local and global model.\n",
    "\n",
    "First naive form of this recomender is filtering based on two threshold:\n",
    "\n",
    "- Absolute number of bikes \"missing\" (a proxy for which is the difference between predictions)\n",
    "- Relative threshold: the difference of prediction divided by the global prediction\n",
    "\n",
    "Exact values of this parameters would depend of technical (and economic) feasibility of bikes supply.\n",
    "Some order of magnitude can be guessed using average daily rental counts.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def recommender(df,count_threshold,pct_threshold):\n",
    "    df[\"pct\"] = np.ceil(df[\"unmet_demand\"])/np.ceil(df[\"global_prediction\"])\n",
    "    return df[(df[\"unmet_demand\"] > count_threshold) & (df[\"pct\"] > pct_threshold)]\n",
    "\n",
    "\n",
    "def add_recomendation(df,count_threshold,pct_threshold):\n",
    "    recomendation_df = recommender(df,count_threshold,pct_threshold)\n",
    "    recomendation_df = recomendation_df[[\"uid\",\"D\"]]\n",
    "    recomendation_df[\"RECOMMENDATION\"] = 1\n",
    "    df = df.merge(recomendation_df,how=\"left\",on=[\"uid\",\"D\"])\n",
    "    df[\"RECOMMENDATION\"] = df[\"RECOMMENDATION\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_prediction(df, uid, dt, cols=[\"rent_count\", \"pred\"]):\n",
    "    condition = (df[\"uid\"] == uid) & (df[\"D\"] == dt)\n",
    "    tmp_df = df[condition]\n",
    "    tmp_df = tmp_df.set_index(\"dt\")\n",
    "    tmp_df[cols].plot.line()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "with_recommendation = add_recomendation(daily_test,25,0.3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final notes\n",
    "\n",
    "What is missing from above reasoning is a way of validating the recomendations.\n",
    "\n",
    "Obvious way as it was stated before would be for instant an experiment where recomendations are applied and number of rented bikes is measured. \n",
    "\n",
    "During the experimentation phase number of bikes added to the station could be signifficantly higher than recommendation to check whether there is some more demand not predicted by the tool."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3810jvsc74a57bd0916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}